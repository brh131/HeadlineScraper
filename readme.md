# Headline Scraper Script
This is a pretty basic news site headline scraping class using urllib for paginated webpages and selenium for continuous scroll webpages. I made this to gather data for a now scrapped machine learning project. This script was made very specifically for the websites I was trying to scrape from but are somewhat extendable to other websites. I do not plan on maintaining this, so use at your own risk, but if these are helpful you are free to use them. There is probably more useful code out there though.
## Required packages
Selenium (I used version 3.141.0, but later versions will probably work)
## WebScraper class
This file contains the base `WebScraper` class, which is a subclass of `html,parser.HTMLParser`. Four of the five example web scrapers included in this repo are instances of `WebScraper`, while the other one is from a subclass inheriting from it. In general, use this class by instantiating it, configuring it by changing fields of the object, then call its scrape method.
### WebScraper.scrape(depth, outputPath)
This function gathers headlines according to the settings described by the object's configuration fields. It continues scraping pages (or screens for continuous scroll websites) until it reaches a number equal to `depth`, or until the number of collected headlines exceeds `resultLimit`. The depth parameter is very imprecise for continuous scroll web pages so I recommend using `resultLimit` to define the length of your scrape instead. Setting `depth` to zero runs the scraper until `resultLimit` is reached. The the headlines are written to the file `outputPath`.
### Configuration fields
- `headlineTag`: the HTML tag that contains the headline. Any text data within that tag (even nested in another tag) will be recorded. If `findHeadlineByClass` is false, then this tag must also be within an \<article> tag  Example: if `headlineTag` is "h2" and \<article>\<h2> \<a> Headline \</a> \</h2>\<\article> is in the HTML,  "Headline" will be recorded.
- `findHeadlineByClass`: if true, the tag containing the headline must be of the class `headlineClass`, and it no longer has to be within an article tag.
- `baseURL`: The URL of the web page your scraping begins on. This will be used to generate the next URL to be scraped in a method based on the value of `nextURLmode`.
- `nextURLmode`: Defines how the next URL to be scraped is generated. The option "continuous" is used for continuous scroll web pages, and a selenium webdriver will be opened and will find more content to scrape by clicking elements of the class `nextLinkClass`. The option "page" is used for strictly paginated webpages (URLs of the form example.com/someSuffix/n for page number n). This sets the next URL to `baseURL` + `pageSuffix` + the page number. The options "append" and "exact" occur when the website is paginated but a link on the page must be used to access the next page. "Append" appends the value in the link to the end of `baseURL` (example: The see more link on The Onion website simply adds a parameter to the end of the URL ?startTime=x to view further back articles). "Exact" is used when the URL in the link is the exact page you wish to scrape next.
- `nextLinkClass`: When the scraper is required to click a link to go to a new page, it must be of this class.
- `pageSuffix`: When `NextURLMode` is "page", this is used to generate the next page. (example.org becomes example.org + pageSuffix + n for the n'th page)
- -`categoryClass`: This is the class of HTML elements that describe the category of the article and is used to exclude articles of undesired categories. 
- `categoryExcludeList`: A list of categories to be excluded. The next headline after an excluded category is not recorded.
- `excludePhrases`: A list of strings. If any headline begins with a string in this list, it is not recorded.
- `resultLimit`: after `resultLimit` headlines are recorded, the program stops scraping after finishing the current page. if `resultLimit` is 0, then there is no limit.
-  `outputFileMode`: This is the mode of the file where the scraped headlines will be written to. See https://docs.python.org/2/library/functions.html#open.
## Included Examples
The included examples "onion scraper.py", huffPoScraper.py, clickHoleScraper.py, and reductressScraper.py simply adjust the configuration fields as needed based on the design of the corresponding website. BuzzfeedScraper.py uses a subclass that implements slightly more functionality to deal with a quirk of the design of the Buzzfeed website. **I do not guarantee that these will be accurate forever.**
